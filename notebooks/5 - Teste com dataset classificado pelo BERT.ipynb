{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste com o dataset classificado pelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from spacy.cli.download import download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_embeddings(df_tweets):\n",
    "    try:\n",
    "        nlp = spacy.load('pt_core_news_lg')\n",
    "    except (IOError, OSError):\n",
    "        download('pt_core_news_lg')\n",
    "        nlp = spacy.load('pt_core_news_lg')\n",
    "    # desativamos todos os outros pipes que vem com o modelo nlp porque não preicsaremos deles\n",
    "    with nlp.disable_pipes():\n",
    "        # transformamos cada texto em um vetor e colocamos em uma array\n",
    "        print('Fazendo os word embeddings')\n",
    "        vetores = np.array([nlp(texto).vector for texto in df_tweets.Texto])\n",
    "\n",
    "    return vetores\n",
    "\n",
    "\n",
    "def ler_modelo(path: str):\n",
    "    return pickle.load(open(path, 'rb'))\n",
    "\n",
    "\n",
    "def salvar_modelo(path: str, modelo):\n",
    "    return pickle.dump(modelo, open(path, 'wb'))    \n",
    "\n",
    "\n",
    "def fazer_amostragem(train_dataset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "\n",
    "    :param train_dataset:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sentimentos = train_dataset['Sentimento'].unique()\n",
    "    df = pd.DataFrame([])\n",
    "\n",
    "    for sentimento in sentimentos:\n",
    "        df_filtrado = train_dataset.loc[train_dataset['Sentimento'] == sentimento][:600]\n",
    "        df = pd.concat([df, df_filtrado])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_datasets = '../resources/datasets'\n",
    "\n",
    "# importação dos dados\n",
    "df_treinamento = pd.read_csv(\n",
    "    f'{path_datasets}/tweets_ekman_2.csv'\n",
    ").dropna()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_treinamento,\n",
    "    df_treinamento['Sentimento'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fazendo os word embeddings\n",
      "Fazendo os word embeddings\n"
     ]
    }
   ],
   "source": [
    "# processamento dos dados para word embeddings\n",
    "path_embeddings_treinamento = '../resources/modelos/embeddings_treinamento_novo.pkl'\n",
    "path_embeddings_teste = '../resources/modelos/embeddings_teste_novo.pkl'\n",
    "\n",
    "if os.path.exists(path_embeddings_treinamento):\n",
    "    embeddings = ler_modelo(path_embeddings_treinamento)\n",
    "else:\n",
    "    embeddings = criar_embeddings(x_train)\n",
    "    salvar_modelo(path_embeddings_treinamento, embeddings)\n",
    "\n",
    "if os.path.exists(path_embeddings_teste):\n",
    "    embeddings_teste = ler_modelo(path_embeddings_teste)\n",
    "else:\n",
    "    embeddings_teste = criar_embeddings(x_test)\n",
    "    salvar_modelo(path_embeddings_teste, embeddings_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sefaz\\Documents\\Projetos Python\\Analise-de-sentimentos-pandemia-covid19\\tweets_venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path_svc = '../resources/modelos/svc.pkl'\n",
    "path_lgr = '../resources/modelos/logistic_regression.pkl'\n",
    "path_forest = '../resources/modelos/forest.pkl'\n",
    "\n",
    "if os.path.exists(path_svc):\n",
    "    svc = ler_modelo(path_svc)\n",
    "else:\n",
    "    svc = LinearSVC(C=100, random_state=0, dual=True, max_iter=10000)\n",
    "    svc.fit(embeddings, y_train)\n",
    "    salvar_modelo(path_svc, svc)\n",
    "    \n",
    "if os.path.exists(path_lgr):\n",
    "    lgr = ler_modelo(path_lgr)\n",
    "else:\n",
    "    lgr = LogisticRegression(random_state=0, max_iter=10000)\n",
    "    lgr.fit(embeddings, y_train)\n",
    "    salvar_modelo(path_lgr, lgr)\n",
    "\n",
    "if os.path.exists(path_forest):\n",
    "    forest = ler_modelo(path_forest)\n",
    "else:\n",
    "    forest = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "    forest.fit(embeddings, y_train)\n",
    "    salvar_modelo(path_forest, forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previsões\n",
    "previsoes_svc = svc.predict(embeddings_teste)\n",
    "previsoes_lgr = lgr.predict(embeddings_teste)\n",
    "previsoes_forest = forest.predict(embeddings_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório Linear SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       feliz       0.59      0.65      0.62        92\n",
      "        medo       0.50      0.69      0.58        51\n",
      "        nojo       0.84      0.61      0.71       220\n",
      "       raiva       0.58      0.60      0.59        58\n",
      "      triste       0.33      0.48      0.39        60\n",
      "\n",
      "    accuracy                           0.61       481\n",
      "   macro avg       0.57      0.61      0.58       481\n",
      "weighted avg       0.66      0.61      0.62       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testar performance\n",
    "print('Relatório Linear SVC')\n",
    "print(classification_report(y_test, previsoes_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       feliz       0.72      0.77      0.75        92\n",
      "        medo       0.71      0.69      0.70        51\n",
      "        nojo       0.83      0.78      0.80       220\n",
      "       raiva       0.59      0.71      0.65        58\n",
      "      triste       0.47      0.47      0.47        60\n",
      "\n",
      "    accuracy                           0.72       481\n",
      "   macro avg       0.67      0.68      0.67       481\n",
      "weighted avg       0.73      0.72      0.72       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Relatório Logistic Regression')\n",
    "print(classification_report(y_test, previsoes_lgr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       feliz       0.64      0.55      0.59        92\n",
      "        medo       0.00      0.00      0.00        51\n",
      "        nojo       0.55      0.93      0.69       220\n",
      "       raiva       0.42      0.09      0.14        58\n",
      "      triste       0.31      0.08      0.13        60\n",
      "\n",
      "    accuracy                           0.55       481\n",
      "   macro avg       0.38      0.33      0.31       481\n",
      "weighted avg       0.46      0.55      0.46       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Relatório Random Forest')\n",
    "print(classification_report(y_test, previsoes_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontuacoes_logreg = cross_val_score(lgr, embeddings_teste, y_test, cv=5, n_jobs=-1, scoring='f1_weighted')\n",
    "pontuacoes_svc = cross_val_score(svc, embeddings_teste, y_test, cv=5, n_jobs=-1, scoring='f1_weighted')\n",
    "pontuacoes_forest = cross_val_score(forest, embeddings_teste, y_test, cv=5, n_jobs=-1, scoring='f1_weighted');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de pontuações: [0.61575121 0.64267069 0.60911738 0.63508692 0.64671752]\n",
      "Média: 0.6298687437279203\n",
      "Lista de pontuações: [0.57669679 0.63381704 0.60378251 0.60058661 0.58953032]\n",
      "Média: 0.6008826524950031\n",
      "Lista de pontuações: [0.35604396 0.46130152 0.43689227 0.40534705 0.50255284]\n",
      "Média: 0.4324275281347279\n"
     ]
    }
   ],
   "source": [
    "def exibir_pontuacoes(pontuacoes):\n",
    "    soma_ponuacoes = 0\n",
    "\n",
    "    for valor in pontuacoes:\n",
    "        soma_ponuacoes += valor\n",
    "        \n",
    "    media = soma_ponuacoes / len(pontuacoes)\n",
    "    \n",
    "    print(f'Lista de pontuações: {pontuacoes}\\nMédia: {media}' )\n",
    "    \n",
    "exibir_pontuacoes(pontuacoes_logreg)\n",
    "exibir_pontuacoes(pontuacoes_svc)\n",
    "exibir_pontuacoes(pontuacoes_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_datasets = '../resources/datasets'\n",
    "\n",
    "# importação dos dados\n",
    "df_treinamento = pd.read_csv(\n",
    "    f'{path_datasets}/tweets_ekman_2.csv'\n",
    ").dropna()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_treinamento,\n",
    "    df_treinamento['Sentimento'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "def carregar_modelo():\n",
    "    path_lgr = '../resources/modelos/logistic_regression.pkl'\n",
    "    path_embeddings_treinamento = '../resources/modelos/embeddings_treinamento_novo.pkl'\n",
    "    path_embeddings_teste = '../resources/modelos/embeddings_teste_novo.pkl'\n",
    "\n",
    "    embeddings = pickle.load(open(path_embeddings_treinamento, 'rb'))\n",
    "    embeddings_teste = pickle.load(open(path_embeddings_teste, 'rb'))\n",
    "    lgr = pickle.load(open(path_lgr, 'rb'))\n",
    "\n",
    "    return lgr\n",
    "\n",
    "\n",
    "def limpar_texto(texto: str):\n",
    "        texto = texto.lower()\n",
    "        texto = re.sub('\\n', '', texto)\n",
    "        texto = re.sub('@\\S+', '', texto)\n",
    "        texto = re.sub(r'https://\\S+', '', texto)\n",
    "\n",
    "        return texto\n",
    "\n",
    "\n",
    "def classificar(texto: str):\n",
    "    nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "    modelo = carregar_modelo()\n",
    "    texto = limpar_texto(texto)\n",
    "\n",
    "    try:\n",
    "        # tokenizamos a frase, vetorizamos e colocamos em uma array\n",
    "        vetor = np.array(nlp(texto).vector)\n",
    "        # colocamos o vetor no formato 1x300, que é o que o modelo espera\n",
    "        vetor = np.reshape(vetor, (1, 300))\n",
    "    except TypeError:\n",
    "        texto = str(texto)\n",
    "        vetor = np.array(nlp(texto).vector)\n",
    "        vetor = np.reshape(vetor, (1, 300))\n",
    "\n",
    "    # fazemos a previsão e retornamos a classe\n",
    "    classes = modelo.classes_\n",
    "    probabilidades = modelo.predict_proba(vetor)\n",
    "\n",
    "    classes_probabilidades = dict({\n",
    "        'classe': list(classes),\n",
    "        'probabilidade': probabilidades.tolist()[0]\n",
    "    })\n",
    "\n",
    "    return classes_probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classe': ['feliz', 'medo', 'nojo', 'raiva', 'triste'],\n",
       " 'probabilidade': [0.1221697759570843,\n",
       "  0.03782998378601986,\n",
       "  0.00496657832294531,\n",
       "  0.8067022114323416,\n",
       "  0.028331450501608905]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificar('Eu tenho q estar grávida p realizarem meus desejos? Só pq eu não estou grávida?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classe': ['feliz', 'medo', 'nojo', 'raiva', 'triste'],\n",
       " 'probabilidade': [0.08405164258977785,\n",
       "  0.0012723040206095429,\n",
       "  0.6822136784088942,\n",
       "  0.00012894465651075164,\n",
       "  0.23233343032420772]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificar('Um relógio de 200 anos, que sobreviveu a monarquia, as oligarquias, ao Varguismo, a ditadura, mas não sobreviveu ao #bolsonarismo, destruíram um #patrimônio inestimável pro país')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classe': ['feliz', 'medo', 'nojo', 'raiva', 'triste'],\n",
       " 'probabilidade': [0.03711378336027173,\n",
       "  0.0005943812155693479,\n",
       "  0.07439056357766399,\n",
       "  0.22921234278030733,\n",
       "  0.6586889290661876]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificar('Parece q namoro um caminhoneiro um carteiro viajante um representante de vendas passa uma semana comigo e outras tantas viajando pra la e pra ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classe': ['feliz', 'medo', 'nojo', 'raiva', 'triste'],\n",
       " 'probabilidade': [0.0002929741663926448,\n",
       "  0.008881476520543741,\n",
       "  0.9401516796761431,\n",
       "  0.0040391538698429244,\n",
       "  0.046634715767077645]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificar('Não é  hora de sensacionalismo, é  hora de unirem forças e  ajudar o povo é não fazer política.  Qnd pararem de querer ganhar em.cima do povo, td pode melhorar. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classe': ['feliz', 'medo', 'nojo', 'raiva', 'triste'],\n",
       " 'probabilidade': [6.244690076693911e-05,\n",
       "  7.965117880749732e-05,\n",
       "  0.5078697012188962,\n",
       "  0.0003546328294471549,\n",
       "  0.4916335678720822]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificar('Acabo de cair num golpe terrível, lamentavelmente abri um pote de sorvete que tava no congelador, e era feijão')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classe': ['feliz', 'medo', 'nojo', 'raiva', 'triste'],\n",
       " 'probabilidade': [0.00021003048980869262,\n",
       "  0.0005743323818542711,\n",
       "  0.5620655232159751,\n",
       "  0.4348512338827211,\n",
       "  0.002298880029640734]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classificar('a choquei nao recebeu nenhuma punição pela postagem fake news da menina, que levou ela a se matar. saiu ilesa do pro processo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
